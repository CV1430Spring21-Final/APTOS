{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet152V2\n",
    "from tensorflow.keras import Sequential, utils, Input, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "# Set constant\n",
    "base_model = ResNet152V2(include_top = False, weights = 'imagenet')\n",
    "preprocess = tf.keras.applications.resnet_v2.preprocess_input\n",
    "IMG_SIZE = (512, 512)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"resized_train_cropped/resized_train_cropped/\"\n",
    "print('number of images in total - ',len(os.listdir(data_path)))\n",
    "data_frame = pd.read_csv(\"trainLabels_cropped.csv\") \n",
    "print(data_frame.head(3))\n",
    "# Getting the additional information from the image name: \n",
    "data_frame['patinet_id'] = data_frame['image'].map(lambda x: x.split('_')[0])\n",
    "#Convert the level into cateorical value: \n",
    "data_frame['level_categorical'] = data_frame['level'].map(lambda x: utils.to_categorical(x, 1 + data_frame['level'].max()))\n",
    "\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "data_frame = pd.concat([data_frame,pd.get_dummies(data_frame['level'], prefix='severity:')],axis=1)\n",
    "data_frame['data_path'] = data_frame['image'].map(lambda x: os.path.join(data_path, f'{x}.jpeg'))\n",
    "print(data_frame.sample(3))\n",
    "data_frame['level'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5:1 train:validation\n",
    "train, validation = train_test_split(data_frame, test_size=0.2)\n",
    "\n",
    "train_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip = True, \n",
    "    preprocessing_function=preprocess\n",
    "    )\n",
    "test_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess)\n",
    "\n",
    "def get_data_set(dataframe, data_generator):\n",
    "    return data_generator.flow_from_dataframe(\n",
    "        dataframe,\n",
    "        directory=None,\n",
    "        x_col=\"data_path\",\n",
    "        y_col='level',\n",
    "        weight_col=None,\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode=\"rgb\",\n",
    "        classes=None,\n",
    "        class_mode=\"raw\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        save_to_dir=None,\n",
    "        save_prefix=\"\",\n",
    "        save_format=\"png\",\n",
    "        subset=None,\n",
    "        interpolation=\"nearest\",\n",
    "        validate_filenames=False,\n",
    "    )\n",
    "train_set = get_data_set(train, train_data_gen)\n",
    "val_set = get_data_set(validation, test_data_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "    \n",
    "image_shape = next(val_set)[0].shape[1:]\n",
    "x0 = Input(image_shape)\n",
    "x = base_model(x0)\n",
    "head = Sequential([\n",
    "    Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(5, activation='softmax', kernel_regularizer='l2')\n",
    "])\n",
    "y = head(x)\n",
    "model = Model(inputs=x0, outputs=y)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = base_model.name + \"-epoch{epoch:02d}-{val_sparse_categorical_accuracy:.2f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=False, mode='auto', save_weights_only=False, save_freq='epoch')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs\", histogram_freq=0, write_graph=True, write_images=True,\n",
    "                               update_freq=\"epoch\", profile_batch=2, embeddings_freq=0, embeddings_metadata=None)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3,\n",
    "                                   verbose=1, mode='auto', min_delta=0.0001, cooldown=5, min_lr=0.0001)\n",
    "earlystop = EarlyStopping(monitor=\"val_loss\", mode=\"auto\", patience=6)\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard, reduceLROnPlat, earlystop]\n",
    "\n",
    "fitting = model.fit(\n",
    "    train_set, \n",
    "    validation_data = val_set, \n",
    "    epochs = 10,\n",
    "    verbose = 1,\n",
    "    callbacks = callbacks_list\n",
    ")\n",
    "# Plot fitting the val_sparse_categorical_accuracy and loss of each epoch\n",
    "print(fitting.history.keys())\n",
    "plt.plot(fitting.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('val_sparse_categorical_accuracy')\n",
    "plt.figure()\n",
    "plt.plot(fitting.history['loss'])\n",
    "plt.title('loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
